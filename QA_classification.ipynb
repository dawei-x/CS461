{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JwE_RpAzLoy",
        "outputId": "47b95916-4a51-4586-9d72-5b9a500152d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files imported\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, BertModel, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "import argparse\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "cs461dir = '/content/drive/My Drive/Colab Notebooks/CS461/'\n",
        "file_paths = ['train_complete.jsonl', 'dev_complete.jsonl', 'test_complete.jsonl']\n",
        "if any(not os.path.exists(filepath) for filepath in file_paths):\n",
        "  !cp -r \"{cs461dir}\"* /content/\n",
        "  print(\"Files imported\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell to load data, SURPPRESSED PRINTING, RECOMMENT IF NEED\n",
        "def load_data():\n",
        "    torch.manual_seed(0)\n",
        "    answers = ['A','B','C','D']\n",
        "\n",
        "    train = []\n",
        "    test = []\n",
        "    valid = []\n",
        "\n",
        "    file_name = 'train_complete.jsonl'\n",
        "    with open(file_name) as json_file:\n",
        "        json_list = list(json_file)\n",
        "    for i in range(len(json_list)):\n",
        "        json_str = json_list[i]\n",
        "        result = json.loads(json_str)\n",
        "\n",
        "        base = result['fact1'] + ' [SEP] ' + result['question']['stem']\n",
        "        ans = answers.index(result['answerKey'])\n",
        "\n",
        "        obs = []\n",
        "        for j in range(4):\n",
        "            text = base + result['question']['choices'][j]['text'] + ' [SEP]'\n",
        "            if j == ans:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "            obs.append([text,label])\n",
        "        train.append(obs)\n",
        "\n",
        "        # print(obs)\n",
        "        # print(' ')\n",
        "\n",
        "        # print(result['question']['stem'])\n",
        "        # print(' ',result['question']['choices'][0]['label'],result['question']['choices'][0]['text'])\n",
        "        # print(' ',result['question']['choices'][1]['label'],result['question']['choices'][1]['text'])\n",
        "        # print(' ',result['question']['choices'][2]['label'],result['question']['choices'][2]['text'])\n",
        "        # print(' ',result['question']['choices'][3]['label'],result['question']['choices'][3]['text'])\n",
        "        # print('  Fact: ',result['fact1'])\n",
        "        # print('  Answer: ',result['answerKey'])\n",
        "        # print('  ')\n",
        "\n",
        "    file_name = 'dev_complete.jsonl'\n",
        "    with open(file_name) as json_file:\n",
        "        json_list = list(json_file)\n",
        "    for i in range(len(json_list)):\n",
        "        json_str = json_list[i]\n",
        "        result = json.loads(json_str)\n",
        "\n",
        "        base = result['fact1'] + ' [SEP] ' + result['question']['stem']\n",
        "        ans = answers.index(result['answerKey'])\n",
        "\n",
        "        obs = []\n",
        "        for j in range(4):\n",
        "            text = base + result['question']['choices'][j]['text'] + ' [SEP]'\n",
        "            if j == ans:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "            obs.append([text,label])\n",
        "        valid.append(obs)\n",
        "\n",
        "    file_name = 'test_complete.jsonl'\n",
        "    with open(file_name) as json_file:\n",
        "        json_list = list(json_file)\n",
        "    for i in range(len(json_list)):\n",
        "        json_str = json_list[i]\n",
        "        result = json.loads(json_str)\n",
        "\n",
        "        base = result['fact1'] + ' [SEP] ' + result['question']['stem']\n",
        "        ans = answers.index(result['answerKey'])\n",
        "\n",
        "        obs = []\n",
        "        for j in range(4):\n",
        "            text = base + result['question']['choices'][j]['text'] + ' [SEP]'\n",
        "            if j == ans:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "            obs.append([text,label])\n",
        "        test.append(obs)\n",
        "\n",
        "    return train, valid, test\n",
        "\n",
        "# Add code to fine-tune and test your MCQA classifier."
      ],
      "metadata": {
        "id": "3yvrcurDzTaj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Approach\n",
        "def train(model, opt):\n",
        "  print(\"training model...\")\n",
        "  model.to(opt.device)\n",
        "  model.train()\n",
        "  opt.classifier.train()\n",
        "\n",
        "  data = opt.train\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  num_samples = len(data)\n",
        "\n",
        "  for epoch in range(opt.epochs):\n",
        "    total_loss = 0.0\n",
        "    random.shuffle(data)\n",
        "\n",
        "    for i in range(0, num_samples, opt.batchsize):\n",
        "      batch = data[i : i + opt.batchsize]\n",
        "      curr_batch_size = len(batch)\n",
        "      texts = []\n",
        "      targets = []\n",
        "\n",
        "      for question in batch:\n",
        "        candidate_texts = [cand[0] for cand in question]\n",
        "        correct_idx = next(idx for idx, cand in enumerate(question) if cand[1] == 1)\n",
        "        texts.extend(candidate_texts)\n",
        "        targets.append(correct_idx)\n",
        "\n",
        "      # tokenize texts\n",
        "      inputs = opt.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "      for input in inputs:\n",
        "        inputs[input] = inputs[input].to(opt.device)\n",
        "\n",
        "      # pass through BERT\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "      # Extract embeddings and then pass through classifier head\n",
        "      embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "      logits = opt.classifier(embeddings)\n",
        "      logits = logits.view(curr_batch_size, 4)\n",
        "\n",
        "      targets_tensor = torch.tensor(targets, device=opt.device)\n",
        "      loss = criterion(logits, targets_tensor)\n",
        "\n",
        "      opt.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      opt.optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / (num_samples / opt.batchsize)\n",
        "    print(f\"Epoch {epoch+1}/{opt.epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "def test(model, opt, file_path='dev_complete.jsonl'):\n",
        "  model.to(opt.device)\n",
        "  opt.classifier.to(opt.device)\n",
        "  model.eval()\n",
        "  opt.classifier.eval()\n",
        "\n",
        "  # need to get and load data here\n",
        "  if file_path == 'dev_complete.jsonl':\n",
        "    data = opt.valid\n",
        "  else:\n",
        "    data = opt.test\n",
        "\n",
        "  num_samples = len(data)/opt.batchsize\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  total_correct = 0\n",
        "  total_loss = 0.0\n",
        "  total_questions = 0\n",
        "\n",
        "  batch_size = opt.batchsize\n",
        "  num_batches = (len(data) + batch_size - 1) // batch_size\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx in range(num_batches):\n",
        "        batch = data[batch_idx * batch_size : (batch_idx+1) * batch_size]\n",
        "        texts = []\n",
        "        targets = []\n",
        "\n",
        "        for question in batch:\n",
        "            candidate_texts = [cand[0] for cand in question]\n",
        "            correct_idx = next(idx for idx, cand in enumerate(question) if cand[1] == 1)\n",
        "            texts.extend(candidate_texts)\n",
        "            targets.append(correct_idx)\n",
        "\n",
        "        inputs = opt.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        for input in inputs:\n",
        "            inputs[input] = inputs[input].to(opt.device)\n",
        "\n",
        "        # pass through BERT\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Extract embeddings and then pass through classifier head\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        logits = opt.classifier(embeddings)\n",
        "\n",
        "        current_batch_size = len(batch)\n",
        "        logits = logits.view(current_batch_size, 4)\n",
        "\n",
        "        targets_tensor = torch.tensor(targets, device=opt.device)\n",
        "        loss = criterion(logits, targets_tensor)\n",
        "        total_loss += loss.item() * current_batch_size\n",
        "\n",
        "        # Compute predictions\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        total_correct += (preds == targets_tensor).sum().item()\n",
        "        total_questions += current_batch_size\n",
        "\n",
        "  avg_loss = total_loss / total_questions\n",
        "  accuracy = total_correct / total_questions\n",
        "  # print(f\"Loss: {avg_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "  return accuracy, avg_loss\n",
        "\n",
        "def main():\n",
        "  random.seed(10)\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('-no_cuda', action='store_true')\n",
        "  parser.add_argument('-SGDR', action='store_true')\n",
        "  parser.add_argument('-epochs', type=int, default=20)\n",
        "  parser.add_argument('-batchsize', type=int, default=16)\n",
        "  parser.add_argument('-printevery', type=int, default=100)\n",
        "  parser.add_argument('-lr', type=float, default=0.00001)\n",
        "  parser.add_argument('-savename', type=str, default=\"bert-base-uncased.pth\")\n",
        "  parser.add_argument('-dir_name', type=str,default='model')\n",
        "  parser.add_argument('-norm', type=float, default=2.0)\n",
        "\n",
        "  if \"google.colab\" in sys.modules:\n",
        "    sys.argv = [\"notebook\"]\n",
        "\n",
        "  opt = parser.parse_args()\n",
        "  opt.verbose = False\n",
        "\n",
        "  opt.device = torch.device(\"cuda\" if torch.cuda.is_available() and not opt.no_cuda else \"cpu\")\n",
        "\n",
        "  time_name = time.strftime(\"%y%m%d_%H%M%S\")\n",
        "  opt.time_name = time_name\n",
        "  dir_name = \"saved/%s\" % (opt.dir_name)\n",
        "  if not os.path.exists(dir_name):\n",
        "      os.makedirs(dir_name)\n",
        "\n",
        "  print(str(opt))\n",
        "  os.makedirs(\"./model_weights\", exist_ok=True)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "  model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "  optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
        "  classifier = nn.Linear(768, 1)\n",
        "  opt.tokenizer = tokenizer\n",
        "  opt.classifier = classifier\n",
        "  opt.optimizer = torch.optim.Adam(list(model.parameters()) + list(classifier.parameters()),\n",
        "                                 lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "  # data is in train, valid, and test, all []\n",
        "  train_data, valid_data, test_data = load_data()\n",
        "  opt.train = train_data\n",
        "  opt.valid = valid_data\n",
        "  opt.test = test_data\n",
        "\n",
        "  print(\"testing model pre-trained model...\")\n",
        "  zero_shot_valid_acc, zero_shot_valid_loss = test(model, opt, file_path='dev_complete.jsonl')\n",
        "  zero_shot_test_acc, zero_shot_test_loss = test(model, opt, file_path='test_complete.jsonl')\n",
        "  print(f'Zero-shot Valid Accuracy: {zero_shot_valid_acc} - Zero-shot Valid Loss: {zero_shot_valid_loss}')\n",
        "  print(f'Zero-shot Test Accuracy: {zero_shot_test_acc} - Zero-shot Test Loss: {zero_shot_test_loss}')\n",
        "  print(\"Now fine-tuning...\")\n",
        "\n",
        "  train(model, opt)\n",
        "\n",
        "  print(\"testing model fine-tuned model...\")\n",
        "  finetune_valid_acc, finetune_valid_loss = test(model, opt, file_path='dev_complete.jsonl')\n",
        "  finetune_test_acc, finetune_test_loss = test(model, opt, file_path='test_complete.jsonl')\n",
        "  print(f'Fine-tune Valid Accuracy: {finetune_valid_acc} - Fine-tune Valid Loss: {finetune_valid_loss}')\n",
        "  print(f'Fine-tune Test Accuracy: {finetune_test_acc} - Fine-tune Test Loss: {finetune_test_loss}')\n",
        "\n",
        "  # save a copy to drive\n",
        "  torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'classifier_state_dict': classifier.state_dict()\n",
        "  }, \"./model_weights/bert-base-uncased.pth\")\n",
        "\n",
        "  shutil.copy(os.path.join(\"./model_weights\", opt.savename), cs461dir)\n",
        "  print(\"Saving model\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPQNrl3QU4s_",
        "outputId": "fb220a79-abbe-4c35-a836-863b80a957f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(no_cuda=False, SGDR=False, epochs=20, batchsize=16, printevery=100, lr=1e-05, savename='bert-base-uncased.pth', dir_name='model', norm=2.0, verbose=False, device=device(type='cuda'), time_name='250305_193844')\n",
            "testing model pre-trained model...\n",
            "Zero-shot Valid Accuracy: 0.276 - Zero-shot Valid Loss: 1.3842072486877441\n",
            "Zero-shot Test Accuracy: 0.272 - Zero-shot Test Loss: 1.3835228042602539\n",
            "Now fine-tuning...\n",
            "training model...\n",
            "Epoch 1/20, Loss: 1.2250\n",
            "Epoch 2/20, Loss: 0.9568\n",
            "Epoch 3/20, Loss: 0.7412\n",
            "Epoch 4/20, Loss: 0.5326\n",
            "Epoch 5/20, Loss: 0.3669\n",
            "Epoch 6/20, Loss: 0.2473\n",
            "Epoch 7/20, Loss: 0.1716\n",
            "Epoch 8/20, Loss: 0.1238\n",
            "Epoch 9/20, Loss: 0.1030\n",
            "Epoch 10/20, Loss: 0.0817\n",
            "Epoch 11/20, Loss: 0.0576\n",
            "Epoch 12/20, Loss: 0.0519\n",
            "Epoch 13/20, Loss: 0.0415\n",
            "Epoch 14/20, Loss: 0.0335\n",
            "Epoch 15/20, Loss: 0.0228\n",
            "Epoch 16/20, Loss: 0.0254\n",
            "Epoch 17/20, Loss: 0.0231\n",
            "Epoch 18/20, Loss: 0.0243\n",
            "Epoch 19/20, Loss: 0.0212\n",
            "Epoch 20/20, Loss: 0.0165\n",
            "testing model fine-tuned model...\n",
            "Fine-tune Valid Accuracy: 0.55 - Fine-tune Valid Loss: 3.016219946861267\n",
            "Fine-tune Test Accuracy: 0.57 - Fine-tune Test Loss: 2.473401246547699\n",
            "Saving model\n"
          ]
        }
      ]
    }
  ]
}